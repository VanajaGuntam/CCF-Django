<!DOCTYPE html>
<html lang="en">
<head>
  <title>Bootstrap Example</title>
  {% load static %}
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <link rel="stylesheet" href="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css">
  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/1.16.0/umd/popper.min.js"></script>
  <script src="https://maxcdn.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js">
  </script>
</head>
<body>

<div class="container">
  <h2>Credit Card Fraud Detection</h2>
  
  <table class="table">
    <thead>
      <tr>
      	<th> </th>>
        <th>Random forest Algorithm </th>
        <th>Decision tree Algorithm </th>
        <th>Logistic Regression </th>
      </tr>
    </thead>
    <tbody>
      <tr>
      	<th> Description</th>
        <td>Random Forest is a popular machine learning algorithm that belongs to the supervised learning technique. It can be used for both Classification and Regression problems in ML. It is based on the concept of ensemble learning, which is a process of combining multiple classifiers to solve a complex problem and to improve the performance of the model.</td>
        <td>Decision Tree is a Supervised learning technique that can be used for both classification and Regression problems, but mostly it is preferred for solving Classification problems. It is a tree-structured classifier, where internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the outcome.</td>
        <td>Logistic regression is one of the most popular Machine Learning algorithms, which comes under the Supervised Learning technique. It is used for predicting the categorical dependent variable using a given set of independent variables.</td>
      </tr>      
      <tr class="table">
      	<th>Dataset</th>
        <td colspan='3'>The datasets contains transactions made by credit cards in September 2013 by european cardholders.
This dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.
It contains only numerical input variables which are the result of a PCA transformation. Unfortunately, due to confidentiality issues, we cannot provide the original features and more background information about the data. Features V1, V2, â€¦ V28 are the principal components obtained with PCA, the only features which have not been transformed with PCA are 'Time' and 'Amount'. Feature 'Time' contains the seconds elapsed between each transaction and the first transaction in the dataset. The feature 'Amount' is the transaction Amount, this feature can be used for example-dependant cost-senstive learning. Feature 'Class' is the response variable and it takes value 1 in case of fraud and 0 otherwise.
</td>
      </tr>
      <tr class="table">
      	<th>Code</th>
      	<td><a href="https://github.com/VanajaGuntam/Credit-card-fraud-detection/blob/master/Creditcard_RandomForestClassifier.ipynb" class="stretched-link">Random Forest code</a></td>
      	<!-- <td>https://github.com/VanajaGuntam/Credit-card-fraud-detection/blob/master/Creditcard_RandomForestClassifier.ipynb</td> -->
        <td><a href="https://github.com/VanajaGuntam/Credit-card-fraud-detection/blob/master/CreditCardFraud_DecisionTreeRegression.ipynb" class="stretched-link">Decision Tree Code</td>
        <td><a href="https://github.com/VanajaGuntam/Credit-card-fraud-detection/blob/master/CreditCardFraud_LogisticRegression.ipynb" class="stretched-link">Logistic Regression</td>
        
      </tr>
      <tr class="table">
      	<th>ROC CURVES</th>
        <td>
  <!-- <link href="/static/img/decisiontree_confusion.png" rel="icon">
 -->
        	<!-- <img src="{% static 'images/decisiontree_confusion.png' %}" width="100%" height="70px"> --><!-- <link href="https://maxcdn.bootstrapcdn.com/bootstrap/3.3.6/css/bootstrap.min.css" rel="stylesheet"/>
<div class="container-fluid">
  <div class="jumbotron">
    <div class="row vertical-align">
      <div class="col-xs-4">
        <a href="#" title=""><img src="http://www.americancivilwarstory.com/images/Coca-Cola_logo.svg.png" class="img-responsive"></a>
      </div>
    <div class="col-xs-4">
        <a href="#" title=""><img src="http://cdn0.sbnation.com/entry_photo_images/2668470/coca-cola_large_verge_medium_landscape.png" class="img-responsive"></a>
    </div>
    <div class="col-xs-4">
        <a href="#" title=""><img src="http://ichef.bbci.co.uk/images/ic/256x256/p03r5406.jpg" class="img-responsive"></a>
    </div>    
  </div>
</div>
</div> -->
        	<img src="/static/images/random_roc.png" class="img-thumbnail"/>
        </td>
        <td><img src="/static/images/decision_roc.png" class="img-thumbnail"/></td>
        <td><img src="/static/images/logistic_roc.png" class="img-thumbnail"/></td>
      </tr>
      <tr class="table">
      	<th>Confusion Matrix</th>
        <td><img src="/static/images/randomforest_confusionmatrix.png" class="img-thumbnail"/></td>
        <td><img src="/static/images/decisiontree_confusion.png" class="img-thumbnail"/></td>
        <td><img src="/static/images/logistic_confusion.png" class="img-thumbnail"/></td>
      </tr>
      <tr class="table">
      	<th>Accuracy</th>
        <td>99.94499</td>
        <td>99.91924</td>
        <td>99.91924</td>
      </tr>
      <tr class="table">
      	<th>Pros</th>
        <td>Random Forests can handle linear and non-linear relationships well.Random Forests work well with both categorical and numerical data. No scaling or transformation of variables is usually necessary.</td>
        <td>Compared to other algorithms decision trees requires less effort for data preparation during pre-processing.A decision tree does not require normalization of data.A decision tree does not require scaling of data as well.</td>
        <td>Logistic regression is easier to implement, interpret, and very efficient to train.It makes no assumptions about distributions of classes in feature space.It can easily extend to multiple classes(multinomial regression) and a natural probabilistic view of class predictions.It is very fast at classifying unknown records.
</td>
      </tr>
      <tr class="table">
      	<th>Cons</th>
        <td>Random Forests can be computationally intensive for large datasets.Random forest is like a black box algorithm, you have very little control over what the model does.Random Forests are not easily interpretable. They provide feature importance but it does not provide complete visibility into the coefficients as linear regression.</td>
        <td>A small change in the data can cause a large change in the structure of the decision tree causing instability.For a Decision tree sometimes calculation can go far more complex compared to other algorithms.</td>
        <td>If the number of observations is lesser than the number of features, Logistic Regression should not be used, otherwise, it may lead to overfitting.It constructs linear boundaries.It can only be used to predict discrete functions. Hence, the dependent variable of Logistic Regression is bound to the discrete number set.
</td>
      </tr>
      
    </tbody>
  </table>
</div>

</body>
</html>

